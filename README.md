
# ğŸ§  LangChain RAG with Ollama, Milvus, Qdrant

This project demonstrates a **Retrieval-Augmented Generation (RAG)** system using **LangChain** and **Ollama**, with pluggable vector databases: **Chroma**, **Milvus**, and **Qdrant**.

---

## ğŸš€ Features

- ğŸ” Document-based Question Answering with LangChain  
- ğŸ’¬ Local LLM support via Ollama (`llama3`, etc.)  
- ğŸ§  Vector DB switchable between Chroma / Milvus / Qdrant  
- ğŸŒ FastAPI backend  
- ğŸ¨ Streamlit web frontend  
- ğŸ³ Dockerized (with Milvus and Qdrant containers)  

---

## ğŸ§± Project Structure

```

â”œâ”€â”€ api/                  # FastAPI backend
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ sample.txt        # Example document
â”œâ”€â”€ vectorstore\_factory.py  # Backend switch logic
â”œâ”€â”€ create\_db.py          # Document ingestion
â”œâ”€â”€ query\_data.py         # CLI QA tool
â”œâ”€â”€ streamlit\_app.py      # Streamlit interface
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

````

---

## âš™ï¸ Quick Start

### 1. Prerequisites

- Python 3.11+
- [Ollama](https://ollama.com) installed and running
- Docker + Docker Compose

---

### 2. Run via Docker

```bash
docker-compose up --build
````

* FastAPI: [http://localhost:8000/ask?question=What%20is%20LangChain\&backend=qdrant](http://localhost:8000/ask?question=What%20is%20LangChain&backend=qdrant)
* Qdrant UI: [http://localhost:6333](http://localhost:6333)
* Milvus: exposed on port `19530` (gRPC)

---

### 3. Run Manually

```bash
# Create DB (Chroma/Milvus/Qdrant)
python create_db.py

# Ask a question
python query_data.py
```

---

### 4. Use Streamlit UI

```bash
streamlit run streamlit_app.py
```

---

## ğŸ” Switch Vector DB

Set the backend as one of:

* `"chroma"`
* `"milvus"`
* `"qdrant"`

You can do this in:

* FastAPI query params
* Streamlit dropdown
* Python code

---

## ğŸ§ª Sample API Call

```http
GET /ask?question=What is LangChain&backend=chroma
```

Response:

```json
{ "answer": "LangChain is a framework..." }
```

---

## ğŸ“¦ Models Used

* LLM: `llama3` via Ollama
* Embedding: `mxbai-embed-large` via Ollama

Pull the models beforehand:

```bash
ollama pull llama3
ollama pull mxbai-embed-large
```

---

## ğŸ›¡ License

MIT â€” feel free to use and extend.